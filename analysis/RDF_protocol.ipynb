{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0509f78d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b1b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MDAnalysis as mda\n",
    "from MDAnalysis.analysis.rdf import InterRDF\n",
    "from MDAnalysis.transformations import NoJump\n",
    "from MDAnalysis.analysis import msd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import csv, re \n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import hann\n",
    "from scipy.interpolate import UnivariateSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d33b5",
   "metadata": {},
   "source": [
    "## Analysis Functions \n",
    "## 1 - RDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function: do RDF from lammps XYZ output as obtained from imput scripts in the MLP simulations section#\n",
    "def do_rdf(file, atom_1='name O', atom_2='name O', nbins=300, rdf_range=(0.0, 10.0), box=12.429):\n",
    "    \"\"\"do RDF from lammps XYZ output as obtained from imput scripts in the MLP simulations section\"\"\"\n",
    "    try:\n",
    "        u = mda.Universe(file, format='XYZ')\n",
    "\n",
    "        # Select only oxygen atoms (assuming '1' denotes oxygens)\n",
    "        for atom in u.atoms:\n",
    "            if atom.name == '2':\n",
    "                atom.name = 'O'\n",
    "                atom.type = 'O'\n",
    "                atom.mass = 15.999\n",
    "            elif atom.name == '1':\n",
    "                atom.name = 'H'\n",
    "                atom.type = 'H'\n",
    "                atom.mass = 1.008\n",
    "\n",
    "        bonds = []\n",
    "        for i in range(0, len(u.atoms), 3):\n",
    "            oxygen = u.atoms[i]\n",
    "            hydrogen1 = u.atoms[i + 1]\n",
    "            hydrogen2 = u.atoms[i + 2]\n",
    "            bonds.extend([(oxygen.index, hydrogen1.index), (oxygen.index, hydrogen2.index)])\n",
    "\n",
    "        u.add_bonds(bonds)\n",
    "        if not np.any(u.dimensions):\n",
    "            u.dimensions = [box, box, box, 90, 90, 90]\n",
    "\n",
    "        sel_1 = u.select_atoms(atom_1)\n",
    "        sel_2 = u.select_atoms(atom_2)\n",
    "        allat = u.select_atoms('all')\n",
    "\n",
    "        # Trajectory unwrapping (ensure proper handling of PBC)\n",
    "        transf = NoJump(allat)\n",
    "        u.trajectory.add_transformations(transf)\n",
    "\n",
    "        # Setup the RDF calculation\n",
    "        rdf_calculator = InterRDF(sel_1, sel_2, range=rdf_range, exclusion_block=(1, 1), nbins=nbins)\n",
    "\n",
    "        # Run the RDF calculation across the trajectory\n",
    "        try:\n",
    "            rdf_calculator.run()\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError in RDF calculation for file {file}: {e}\")\n",
    "            return None  # Skip problematic file\n",
    "\n",
    "        return rdf_calculator\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "        return None\n",
    "#Example Usage \n",
    "#DFT_RDF        = do_rdf('trajectory.xyz',box=24.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158bc7f9",
   "metadata": {},
   "source": [
    "## Analysis Functions \n",
    "## 2 - Diffiusion Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diffusion_coefficient(\n",
    "    xyz_files,\n",
    "    box_length_A,\n",
    "    start_ps,\n",
    "    end_ps,\n",
    "    timestep_ps=0.1,\n",
    "    selection=\"name O\",\n",
    "    dim_factor=3,\n",
    "    temperature=300.0,\n",
    "    viscosity=0.89e-3,\n",
    "    xi=2.837297,\n",
    "    n_bootstrap=1000,\n",
    "    plot_msd=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the diffusion coefficient from a list of .xyz files using MDAnalysis,\n",
    "    including a finite-size (Yeh–Hummer) correction and bootstrap error estimation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz_files : list of str\n",
    "        Paths to the .xyz trajectory files.\n",
    "    box_length_A : float\n",
    "        Cubic box length in Angstroms (Å).\n",
    "    start_ps : float\n",
    "        Start time (ps) of the linear fit window (in the diffusive regime).\n",
    "    end_ps : float\n",
    "        End time (ps) of the linear fit window.\n",
    "    timestep_ps : float, optional\n",
    "        Time step between frames in ps (default 0.1 ps).\n",
    "    selection : str, optional\n",
    "        Atom selection for MSD calculation (default \"name O\").\n",
    "    dim_factor : int, optional\n",
    "        Dimensional factor for the Einstein relation (3 for 3D, 2 for 2D, etc.).\n",
    "    temperature : float, optional\n",
    "        Temperature in K for the finite-size correction (default 300 K).\n",
    "    viscosity : float, optional\n",
    "        Fluid viscosity in Pa·s (default 0.89e-3, typical for water at ~300 K).\n",
    "    xi : float, optional\n",
    "        Geometry constant for cubic boxes in the Yeh–Hummer correction (default 2.837297).\n",
    "    n_bootstrap : int, optional\n",
    "        Number of bootstrap resamples (default 1000).\n",
    "    plot_msd : bool, optional\n",
    "        If True, plot both the individual trajectory MSDs and the overall averaged MSD (default True).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        A dictionary containing:\n",
    "        - \"D_raw_A2_ps\": Diffusion coefficient from the linear fit in Å^2/ps\n",
    "        - \"D_raw_err_A2_ps\": Uncertainty in D_raw in Å^2/ps (from bootstrap)\n",
    "        - \"D_infinity_m2_s\": Finite-size corrected D in m^2/s\n",
    "        - \"D_infinity_err_m2_s\": Uncertainty in corrected D in m^2/s\n",
    "        - \"avg_msd\": Array of the averaged MSD (Å^2)\n",
    "        - \"lagtimes\": Array of time points (ps) for the MSD\n",
    "    \"\"\"\n",
    "    # ------------------\n",
    "    # 1) Read trajectories, compute MSD\n",
    "    # ------------------\n",
    "    all_msds = []\n",
    "    frame_counts = []\n",
    "\n",
    "    for xyz in xyz_files:\n",
    "        # Create universe from XYZ\n",
    "        u = mda.Universe(xyz, format='XYZ')\n",
    "        \n",
    "        # Rename atoms\n",
    "        for atom in u.atoms:\n",
    "            if atom.name == '2':\n",
    "                atom.name = 'O'\n",
    "                atom.type = 'O'\n",
    "                atom.mass = 15.999\n",
    "            elif atom.name == '1':\n",
    "                atom.name = 'H'\n",
    "                atom.type = 'H'\n",
    "                atom.mass = 1.008\n",
    "        \n",
    "        # Add bonds for each water (3 atoms)\n",
    "        bonds = []\n",
    "        for i in range(0, len(u.atoms), 3):\n",
    "            oxygen = u.atoms[i]\n",
    "            hydrogen1 = u.atoms[i+1]\n",
    "            hydrogen2 = u.atoms[i+2]\n",
    "            bonds.extend([\n",
    "                (oxygen.index, hydrogen1.index),\n",
    "                (oxygen.index, hydrogen2.index)\n",
    "            ])\n",
    "        u.add_bonds(bonds)\n",
    "        \n",
    "        # Set dimensions if missing\n",
    "        if not np.any(u.dimensions):\n",
    "            u.dimensions = [box_length_A, box_length_A, box_length_A, 90, 90, 90]\n",
    "        \n",
    "        # Apply NoJump unwrapping\n",
    "        u.trajectory.add_transformations(NoJump())\n",
    "        \n",
    "        # Compute MSD for selection\n",
    "        msd_analysis = msd.EinsteinMSD(\n",
    "            u,\n",
    "            select=selection,\n",
    "            msd_type=\"xyz\",\n",
    "            fft=True\n",
    "        )\n",
    "        msd_analysis.run()\n",
    "        \n",
    "        # (n_frames, n_particles)\n",
    "        msds_array = msd_analysis.results.msds_by_particle\n",
    "        all_msds.append(msds_array)\n",
    "        frame_counts.append(msds_array.shape[0])\n",
    "\n",
    "    # Truncate all MSD arrays to the length of the shortest trajectory\n",
    "    min_frames = min(frame_counts)\n",
    "    all_msds_trunc = [arr[:min_frames, :] for arr in all_msds]\n",
    "\n",
    "    # Combine replicate MSDs (concatenate along axis=1)\n",
    "    combined_msds = np.concatenate(all_msds_trunc, axis=1)\n",
    "\n",
    "    # Average over all particles (across all trajectories)\n",
    "    avg_msd = np.mean(combined_msds, axis=1)  # shape: (min_frames,)\n",
    "    lagtimes = np.arange(min_frames) * timestep_ps\n",
    "\n",
    "    # ------------------\n",
    "    # Plotting: individual trajectories and overall average\n",
    "    # ------------------\n",
    "    if plot_msd:\n",
    "        plt.figure()\n",
    "        # Plot each trajectory's average MSD (averaging over particles in that trajectory)\n",
    "        for i, msd_arr in enumerate(all_msds_trunc):\n",
    "            traj_avg = np.mean(msd_arr, axis=1)\n",
    "            plt.plot(lagtimes, traj_avg, linestyle='--', alpha=0.7, label=f\"Trajectory {i+1}\")\n",
    "        # Overlay the overall average MSD\n",
    "        plt.plot(lagtimes, avg_msd, color='black', linewidth=2, label=\"Overall Average\")\n",
    "        plt.xlabel(\"Lag time (ps)\")\n",
    "        plt.ylabel(\"MSD (Å$^2$)\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # ------------------\n",
    "    # 2) Linear Fit in the Diffusive Regime\n",
    "    # ------------------\n",
    "    start_idx = np.searchsorted(lagtimes, start_ps)\n",
    "    end_idx = np.searchsorted(lagtimes, end_ps)\n",
    "    \n",
    "    # Ordinary least squares on the average\n",
    "    lr = linregress(lagtimes[start_idx:end_idx], avg_msd[start_idx:end_idx])\n",
    "    slope = lr.slope\n",
    "    # D in Å^2/ps\n",
    "    D_raw_A2_ps = slope / (2.0 * dim_factor)\n",
    "\n",
    "    # ------------------\n",
    "    # 3) Bootstrap to get error\n",
    "    # ------------------\n",
    "    n_particles = combined_msds.shape[1]\n",
    "    bootstrap_slopes = np.zeros(n_bootstrap)\n",
    "\n",
    "    for i in range(n_bootstrap):\n",
    "        # Resample with replacement among particles\n",
    "        sample_indices = np.random.choice(n_particles, n_particles, replace=True)\n",
    "        boot_avg_msd = np.mean(combined_msds[:, sample_indices], axis=1)\n",
    "        \n",
    "        lr_boot = linregress(lagtimes[start_idx:end_idx], boot_avg_msd[start_idx:end_idx])\n",
    "        bootstrap_slopes[i] = lr_boot.slope\n",
    "\n",
    "    # Mean slope and std from bootstrap\n",
    "    slope_mean = np.mean(bootstrap_slopes)\n",
    "    slope_std = np.std(bootstrap_slopes)\n",
    "\n",
    "    D_bootstrap_A2_ps = slope_mean / (2.0 * dim_factor)\n",
    "    D_bootstrap_err_A2_ps = slope_std / (2.0 * dim_factor)\n",
    "\n",
    "    # ------------------\n",
    "    # 4) Finite-Size Correction (Yeh–Hummer)\n",
    "    # ------------------\n",
    "    # Convert from Å^2/ps to m^2/s\n",
    "    # 1 Å = 1e-10 m, 1 ps = 1e-12 s => 1 (Å^2/ps) = 1e-8 (m^2/s)\n",
    "    D_raw_m2_s = D_bootstrap_A2_ps * 1.0e-8\n",
    "    D_raw_err_m2_s = D_bootstrap_err_A2_ps * 1.0e-8\n",
    "\n",
    "    # Box length in meters\n",
    "    L_m = box_length_A * 1.0e-10\n",
    "\n",
    "    # k_B in J/K\n",
    "    kB = 1.380649e-23\n",
    "    beta = 1.0 / (kB * temperature)\n",
    "\n",
    "    # Yeh–Hummer correction term: D(∞) = D(L) + xi/(6π η β L)\n",
    "    D_correction_m2_s = xi / (6.0 * np.pi * viscosity * beta * L_m)\n",
    "\n",
    "    # Final corrected D\n",
    "    D_infinity_m2_s = D_raw_m2_s + D_correction_m2_s\n",
    "\n",
    "    # Assuming the correction is exact (no error in T, eta, L, xi),\n",
    "    # the error in D(∞) is the same as that in D(L).\n",
    "    D_infinity_err_m2_s = D_raw_err_m2_s\n",
    "\n",
    "    # ------------------\n",
    "    # 5) Prepare Results\n",
    "    # ------------------\n",
    "    results = {\n",
    "        \"D_raw_A2_ps\": D_bootstrap_A2_ps,\n",
    "        \"D_raw_err_A2_ps\": D_bootstrap_err_A2_ps,\n",
    "        \"D_infinity_m2_s\": D_infinity_m2_s,\n",
    "        \"D_infinity_err_m2_s\": D_infinity_err_m2_s,\n",
    "        \"avg_msd\": avg_msd,\n",
    "        \"lagtimes\": lagtimes\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print(\"=== Diffusion Coefficient Results ===\")\n",
    "    print(f\"Raw D(L) in Å^2/ps = {D_bootstrap_A2_ps:.5f} ± {D_bootstrap_err_A2_ps:.5f}\")\n",
    "    print(f\"Raw D(L) in m^2/s  = {D_raw_m2_s:.4e} ± {D_raw_err_m2_s:.4e}\")\n",
    "    print(f\"Correction term    = {D_correction_m2_s:.4e} m^2/s\")\n",
    "    print(f\"D(∞) in m^2/s      = {D_infinity_m2_s:.4e} ± {D_infinity_err_m2_s:.4e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "#xyz_files = [f'run_nve_{i}.xyz' for i in range(1,11)]\n",
    "#\n",
    "# Suppose your box is 25 Å on each side\n",
    "# box_length_A = 25\n",
    "\n",
    "# We'll fit from 10 ps to 50 ps\n",
    "#start_ps = 10.0\n",
    "#end_ps   = 50.0\n",
    "#\n",
    "#results = compute_diffusion_coefficient(\n",
    "#    xyz_files,\n",
    "#    box_length_A,\n",
    "#    start_ps,\n",
    "#    end_ps,\n",
    "#    timestep_ps=0.5,\n",
    "#    selection=\"name O\",\n",
    "#    dim_factor=3,\n",
    "#    temperature=300.0,\n",
    "#    viscosity=0.89e-3,\n",
    "#    xi=2.837297,\n",
    "#    n_bootstrap=1000,\n",
    "#    plot_msd=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91512d",
   "metadata": {},
   "source": [
    "## Analysis Functions \n",
    "## 3 - Fusion and Vaporization Enthalpies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83c814be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lammps_log(filename, time_step=None):\n",
    "    data_lines = []\n",
    "    header = None\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i]\n",
    "            if line.strip().startswith('Step'):\n",
    "                header = line.strip().split()\n",
    "                i += 1\n",
    "                while i < len(lines):\n",
    "                    line = lines[i].strip()\n",
    "                    if not line:\n",
    "                        break\n",
    "                    tokens = line.split()\n",
    "                    if len(tokens) != len(header):\n",
    "                        break\n",
    "                    try:\n",
    "                        data_line = [float(token) for token in tokens]\n",
    "                        data_lines.append(data_line)\n",
    "                    except ValueError:\n",
    "                        break\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "    if not data_lines:\n",
    "        raise ValueError('No data found in the log file.')\n",
    "    data = np.array(data_lines)\n",
    "    if 'Time' not in header:\n",
    "        if time_step is None:\n",
    "            time_step = 0.0005  # ps per step; adjust if necessary\n",
    "        if 'Step' not in header:\n",
    "            raise ValueError(\"'Step' column not found in data.\")\n",
    "        step_idx = header.index('Step')\n",
    "        time_array = data[:, step_idx] * time_step\n",
    "        header.append('Time')\n",
    "        data = np.hstack((data, time_array.reshape(-1, 1)))\n",
    "    return header, data\n",
    "\n",
    "def read_ase_log(filename):\n",
    "    data_lines = []\n",
    "    header = ['Step', 'Time', 'Temp', 'Press', 'Density', 'E_pair', 'TotEng']\n",
    "    time_step = 0.0005  # ps per step; adjust if necessary\n",
    "    step_offset = 0\n",
    "    prev_step = None\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('Step:'):\n",
    "                line = re.sub(r'([a-zA-Z]):', r'\\1: ', line)\n",
    "                line = re.sub(r'([a-zA-Z])([0-9])', r'\\1 \\2', line)\n",
    "                parts = line.split(',')\n",
    "                data_dict = {}\n",
    "                for part in parts:\n",
    "                    if ':' in part:\n",
    "                        key, value = part.split(':', 1)\n",
    "                        key = key.strip()\n",
    "                        value = value.strip().split()[0]\n",
    "                        try:\n",
    "                            data_dict[key] = float(value)\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                current_step = data_dict.get('Step', 0)\n",
    "                if prev_step is not None and current_step < prev_step:\n",
    "                    step_offset += prev_step + 1  # adjust for restart\n",
    "                adjusted_step = current_step + step_offset\n",
    "                prev_step = current_step\n",
    "                time_val = adjusted_step * time_step\n",
    "                density = data_dict.get('Density', np.nan)\n",
    "                data_line = [\n",
    "                    adjusted_step,\n",
    "                    time_val,\n",
    "                    data_dict.get('Temperature', np.nan),\n",
    "                    data_dict.get('Pressure', np.nan),\n",
    "                    density,\n",
    "                    data_dict.get('Mace Energy', np.nan),\n",
    "                    data_dict.get('Total Energy', np.nan)\n",
    "                ]\n",
    "                data_lines.append(data_line)\n",
    "    if not data_lines:\n",
    "        raise ValueError('No data found in the ASE log file.')\n",
    "    data = np.array(data_lines)\n",
    "    return header, data\n",
    "\n",
    "def compute_average_enthalpy(log_type, filename, energy_column, time_step=0.0005,\n",
    "                             conversion_factor=6.24e-7, ase_mass=None, eq_window=None,\n",
    "                             num_molecules=None):\n",
    "    \"\"\"\n",
    "    Compute the average enthalpy from a log file and convert the result to meV/mol.\n",
    "\n",
    "    Parameters:\n",
    "      log_type (str): Either 'lammps' or 'ase'.\n",
    "      filename (str): Path to the log file.\n",
    "      energy_column (str): Name of the energy column (e.g. 'TotEng').\n",
    "      time_step (float): Time per step (ps) if not present.\n",
    "      conversion_factor (float): Converts (bar * Å³) to eV.\n",
    "      ase_mass (float or None): For ASE logs without a Volume column, use mass to compute V = mass / Density.\n",
    "      eq_window (float or None): Equilibrium window (in ps) over which to average. If None, average over all data.\n",
    "      num_molecules (int or None): Number of molecules in the simulation box (required for conversion to meV/mol).\n",
    "\n",
    "    Returns:\n",
    "      avg_enthalpy_meVmol (float): The average enthalpy in meV/mol.\n",
    "    \"\"\"\n",
    "    # Parse the file\n",
    "    if log_type == 'lammps':\n",
    "        header, data = read_lammps_log(filename, time_step)\n",
    "    elif log_type == 'ase':\n",
    "        header, data = read_ase_log(filename)\n",
    "        print(header)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported log type.\")\n",
    "\n",
    "    # Map header names to indices.\n",
    "    col_idx = {name: idx for idx, name in enumerate(header)}\n",
    "    \n",
    "    # Check for required columns.\n",
    "    if 'Press' not in col_idx:\n",
    "        raise ValueError(\"Column 'Press' not found.\")\n",
    "    if energy_column not in col_idx:\n",
    "        raise ValueError(f\"Energy column '{energy_column}' not found.\")\n",
    "    \n",
    "    # Extract pressure (in bar) and energy (in eV).\n",
    "    press_data = data[:, col_idx['Press']]\n",
    "    energy_data = data[:, col_idx[energy_column]]\n",
    "    \n",
    "    # Determine volume: use 'Volume' if available; otherwise, compute from Density.\n",
    "    if 'Volume' in col_idx:\n",
    "        volume_data = data[:, col_idx['Volume']]\n",
    "    elif 'Density' in col_idx:\n",
    "        if ase_mass is None:\n",
    "            raise ValueError(\"ASE log provided without 'Volume'. Please supply ase_mass to compute volume from Density.\")\n",
    "        density_data = data[:, col_idx['Density']]\n",
    "        volume_data = ase_mass / density_data\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'Volume' nor 'Density' found in the data.\")\n",
    "    \n",
    "    # Compute the instantaneous enthalpy for each snapshot: H = E + (P*V)*conversion_factor (in eV/box).\n",
    "    enthalpy = energy_data + press_data * volume_data * conversion_factor\n",
    "    \n",
    "    # Get the time array.\n",
    "    if 'Time' in col_idx:\n",
    "        time_data = data[:, col_idx['Time']]\n",
    "    elif 'Step' in col_idx:\n",
    "        time_data = data[:, col_idx['Step']] * time_step\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'Time' nor 'Step' column found.\")\n",
    "    \n",
    "    # Average over the equilibrium window if provided.\n",
    "    if eq_window is not None:\n",
    "        total_time = time_data[-1]\n",
    "        eq_mask = time_data >= (total_time - eq_window)\n",
    "        avg_enthalpy = np.mean(enthalpy[eq_mask])\n",
    "    else:\n",
    "        avg_enthalpy = np.mean(enthalpy)\n",
    "    \n",
    "    # Convert from eV/box to meV/mol.\n",
    "    if num_molecules is None:\n",
    "        raise ValueError(\"num_molecules must be provided for conversion to meV/mol.\")\n",
    "    # Average per molecule in eV.\n",
    "    avg_per_molecule = avg_enthalpy / num_molecules\n",
    "    # Convert to eV per mole.\n",
    "    NA = 6.02214076e23  # Avogadro's number (molecules/mol)\n",
    "    avg_per_mole_eV = avg_per_molecule \n",
    "    # Convert to meV.\n",
    "    avg_enthalpy_meVmol = avg_per_mole_eV * 1000\n",
    "    \n",
    "    return avg_enthalpy_meVmol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7563e57",
   "metadata": {},
   "source": [
    "## Analysis Functions \n",
    "## 4 - Equilibrium Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd3f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lammps_log(filename, time_step=None):\n",
    "    \"\"\"\n",
    "    Reads a LAMMPS log file and extracts the numerical data.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Path to the log file.\n",
    "    time_step (float): Time per step (in ps), optional.\n",
    "\n",
    "    Returns:\n",
    "    header (list of str): List of column names.\n",
    "    data (np.ndarray): 2D numpy array of the numerical data.\n",
    "    \"\"\"\n",
    "    data_lines = []\n",
    "    header = None\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i]\n",
    "            if line.strip().startswith('Step'):\n",
    "                header_line = line\n",
    "                header = line.strip().split()\n",
    "                data_start_index = i + 1\n",
    "                i += 1\n",
    "                # Read data lines until we hit an empty line or a non-data line\n",
    "                while i < len(lines):\n",
    "                    line = lines[i].strip()\n",
    "                    if not line:\n",
    "                        break  # Empty line signifies end of data block\n",
    "                    tokens = line.split()\n",
    "                    # Check if the number of tokens matches the header length\n",
    "                    if len(tokens) != len(header):\n",
    "                        break  # Likely end of data block\n",
    "                    try:\n",
    "                        data_line = [float(token) for token in tokens]\n",
    "                        data_lines.append(data_line)\n",
    "                    except ValueError:\n",
    "                        break  # Non-numeric line, end of data block\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "    if not data_lines:\n",
    "        raise ValueError('No data found in the log file.')\n",
    "    data = np.array(data_lines)\n",
    "    # Now, check if 'Time' is in header, if not, compute it\n",
    "    if 'Time' not in header:\n",
    "        if time_step is None:\n",
    "            time_step = 0.0005  # Adjust as necessary based on your simulation parameters\n",
    "        # Compute 'Time' from 'Step'\n",
    "        if 'Step' not in header:\n",
    "            raise ValueError(\"'Step' column not found in data.\")\n",
    "        step_idx = header.index('Step')\n",
    "        time_array = data[:, step_idx] * time_step\n",
    "        # Add 'Time' column to header and data\n",
    "        header.append('Time')\n",
    "        data = np.hstack((data, time_array.reshape(-1, 1)))\n",
    "    return header, data\n",
    "\n",
    "def read_ase_log(filename):\n",
    "    \"\"\"\n",
    "    Reads an ASE log file and extracts the numerical data, adjusting 'Step' values to be continuous across restarts.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Path to the ASE log file.\n",
    "\n",
    "    Returns:\n",
    "    header (list of str): List of column names.\n",
    "    data (np.ndarray): 2D numpy array of the numerical data.\n",
    "    \"\"\"\n",
    "    data_lines = []\n",
    "    header = ['Step', 'Time', 'Temp', 'Press', 'Density', 'TotEng']\n",
    "    time_step = 0.0005  # Adjust based on your actual time step (ps per step)\n",
    "    step_offset = 0\n",
    "    prev_step = None\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line_num, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if line.startswith('Step:'):\n",
    "                # Replace missing spaces or commas\n",
    "                line = re.sub(r'([a-zA-Z]):', r'\\1: ', line)\n",
    "                line = re.sub(r'([a-zA-Z])([0-9])', r'\\1 \\2', line)\n",
    "                # Split the line into key-value pairs\n",
    "                parts = line.split(',')\n",
    "                data_dict = {}\n",
    "                for part in parts:\n",
    "                    if ':' in part:\n",
    "                        key, value = part.split(':', 1)\n",
    "                        key = key.strip()\n",
    "                        value = value.strip()\n",
    "                        # Remove units and extract numerical value\n",
    "                        value = value.split()[0]\n",
    "                        try:\n",
    "                            data_dict[key] = float(value)\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                # Parse current step\n",
    "                current_step = data_dict.get('Step', 0)\n",
    "                # Check for restart (Step reset)\n",
    "                if prev_step is not None and current_step < prev_step:\n",
    "                    # Restart detected\n",
    "                    step_offset += prev_step + 1  # Add offset\n",
    "                # Adjust step\n",
    "                adjusted_step = current_step + step_offset\n",
    "                prev_step = current_step\n",
    "                # Calculate time\n",
    "                time = adjusted_step * time_step\n",
    "                # Handle density unit conversion (if needed)\n",
    "                density = data_dict.get('Density', np.nan)\n",
    "                if not np.isnan(density):\n",
    "                    # Assuming ASE density is in correct units already\n",
    "                    density = density \n",
    "                data_line = [\n",
    "                    adjusted_step,\n",
    "                    time,\n",
    "                    data_dict.get('Temperature', np.nan),\n",
    "                    data_dict.get('Pressure', np.nan),\n",
    "                    density,\n",
    "                    data_dict.get('Total Energy', np.nan)\n",
    "                ]\n",
    "                data_lines.append(data_line)\n",
    "    if not data_lines:\n",
    "        raise ValueError('No data found in the ASE log file.')\n",
    "    data = np.array(data_lines)\n",
    "    return header, data\n",
    "\n",
    "def running_mean_std(data, window_size):\n",
    "    \"\"\"\n",
    "    Calculate the running mean and standard deviation using a sliding window.\n",
    "\n",
    "    Parameters:\n",
    "    data (np.ndarray): 1D array of data points\n",
    "    window_size (int): Size of the sliding window\n",
    "\n",
    "    Returns:\n",
    "    mean (np.ndarray): Running mean\n",
    "    std (np.ndarray): Running standard deviation\n",
    "    \"\"\"\n",
    "    cumsum = np.cumsum(np.insert(data, 0, 0))\n",
    "    cumsum_sq = np.cumsum(np.insert(data**2, 0, 0))\n",
    "\n",
    "    mean = (cumsum[window_size:] - cumsum[:-window_size]) / window_size\n",
    "    mean_sq = (cumsum_sq[window_size:] - cumsum_sq[:-window_size]) / window_size\n",
    "    variance = mean_sq - mean**2\n",
    "    variance[variance < 0] = 0  # Correct for small negative variances due to floating-point errors\n",
    "    std = np.sqrt(variance)\n",
    "    return mean, std\n",
    "def analyze_and_plot_logs(log_files, window_size, column_to_analyze, labels, colors, output_filename, plot_title, \n",
    "                              eq_window, total_sim_time):\n",
    "    \"\"\"\n",
    "    Reads multiple log files (LAMMPS and ASE), calculates running mean and std for specified column,\n",
    "    and plots the results. Also computes an equilibrium average over a specified time window.\n",
    "\n",
    "    Parameters:\n",
    "    log_files (list of tuples): List of tuples containing the log type ('lammps' or 'ase') and file path.\n",
    "    window_size (int): Window size for running mean and std.\n",
    "    column_to_analyze (str): Name of the column to analyze (e.g., 'Press', 'Density').\n",
    "    labels (list of str): Labels for each dataset (used in the legend).\n",
    "    colors (list of str): Colors for each dataset.\n",
    "    output_filename (str): Filename to save the plot.\n",
    "    plot_title (str): Title of the plot.\n",
    "    eq_window (float): Time window (in ps) at end of simulation for equilibrium calculation.\n",
    "    total_sim_time (float): Total simulation time (in ps).\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i, (log_type, filename) in enumerate(log_files):\n",
    "        if log_type == 'lammps':\n",
    "            header, data = read_lammps_log(filename)\n",
    "        elif log_type == 'ase':\n",
    "            header, data = read_ase_log(filename)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown log type '{log_type}' for file '{filename}'\")\n",
    "        data_list.append((header, data))\n",
    "\n",
    "    adjusted_time_arrays = []\n",
    "    mean_arrays = []\n",
    "    std_arrays = []\n",
    "    eq_means = []\n",
    "    eq_stds = []\n",
    "\n",
    "    for i, (header, data) in enumerate(data_list):\n",
    "        column_indices = {name: idx for idx, name in enumerate(header)}\n",
    "        if column_to_analyze not in column_indices:\n",
    "            raise ValueError(f\"Column '{column_to_analyze}' not found in data for dataset '{labels[i]}'\")\n",
    "\n",
    "        # Determine time array\n",
    "        if 'Time' in column_indices:\n",
    "            time_data = data[:, column_indices['Time']]\n",
    "        else:\n",
    "            if 'Step' not in column_indices:\n",
    "                raise ValueError(f\"'Time' or 'Step' column not found in data for dataset '{labels[i]}'\")\n",
    "            time_step = 0.0005\n",
    "            time_data = data[:, column_indices['Step']] * time_step\n",
    "\n",
    "        col_data = data[:, column_indices[column_to_analyze]]\n",
    "        if len(col_data) < window_size:\n",
    "            raise ValueError(f\"Not enough data points in '{labels[i]}' for the given window size.\")\n",
    "\n",
    "        # Compute running mean and std\n",
    "        mean_col, std_col = running_mean_std(col_data, window_size)\n",
    "        mean_arrays.append(mean_col)\n",
    "        std_arrays.append(std_col)\n",
    "        adjusted_time_arrays.append(time_data[window_size - 1:])\n",
    "\n",
    "        # Equilibrium calculation\n",
    "        eq_start_time = total_sim_time - eq_window\n",
    "        eq_mask = time_data >= eq_start_time\n",
    "        eq_data = col_data[eq_mask]\n",
    "        if eq_data.size > 0:\n",
    "            eq_mean = np.mean(eq_data)\n",
    "            eq_std = np.std(eq_data)\n",
    "        else:\n",
    "            eq_mean = np.nan\n",
    "            eq_std = np.nan\n",
    "        eq_means.append(eq_mean)\n",
    "        eq_stds.append(eq_std)\n",
    "\n",
    "        # Print equilibrium values\n",
    "        print(f\"Equilibrium stats for '{labels[i]}': mean = {eq_mean:.4f}, std = {eq_std:.4f}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure()\n",
    "    for i in range(len(mean_arrays)):\n",
    "        time_arr = adjusted_time_arrays[i]\n",
    "        plt.plot(time_arr, mean_arrays[i], label=labels[i], color=colors[i])\n",
    "        plt.fill_between(time_arr,\n",
    "                         mean_arrays[i] - std_arrays[i],\n",
    "                         mean_arrays[i] + std_arrays[i],\n",
    "                         color=colors[i], alpha=0.2)\n",
    "        # Equilibrium line\n",
    "        eq_start_time = total_sim_time - eq_window\n",
    "        plt.hlines(eq_means[i], eq_start_time, total_sim_time,\n",
    "                   colors=colors[i], linestyles='--',\n",
    "                   label=f\"{labels[i]} eq mean = {eq_means[i]:.4f} ± {eq_stds[i]:.4f}\")\n",
    "\n",
    "    plt.xlabel('Time (ps)')\n",
    "    plt.ylabel(column_to_analyze)\n",
    "    plt.title(plot_title, size=14)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlim(0, max(arr[-1] for arr in adjusted_time_arrays))\n",
    "    plt.savefig(output_filename, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50f5fb",
   "metadata": {},
   "source": [
    "## Analysis Functions \n",
    "## 5 - Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b04b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_autocorrelation_from_csv(csv_filename):\n",
    "    \"\"\"\n",
    "    Loads the CSV file containing velocity auto-correlation function \n",
    "    as obtained from the do_vacf.py script.\n",
    "    \"\"\"\n",
    "    freq_vals = []\n",
    "    T_vals = []\n",
    "    VACF_vals = []\n",
    "\n",
    "    # 1) Read data from CSV\n",
    "    with open(csv_filename, 'r', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)  # skip the header row\n",
    "        # Expect something like [\"Frequency(cm^-1)\", \"GaussSpectrum\", \"HannSpectrum\"]\n",
    "\n",
    "        for row in reader:\n",
    "            # Each 'row' is a list of strings from that line\n",
    "            # Convert them to float\n",
    "            T_vals.append(float(row[0]))\n",
    "            VACF_vals.append(float(row[1]))\n",
    "\n",
    "    # 2) Convert to NumPy arrays (not strictly required, but often convenient)\n",
    "    T_vals = np.array(freq_vals)\n",
    "    VACF_vals = np.array(VACF_vals)\n",
    "    return T_vals, VACF_vals\n",
    "\n",
    "def hann_windowed_fft_real_power(vacf, dt_fs, truncation_length=None):\n",
    "    \"\"\"\n",
    "    Applies Hann window to a truncated VACF segment and computes FFT.\n",
    "    \n",
    "    Parameters:\n",
    "    - vacf: 1D numpy array of the velocity autocorrelation function\n",
    "    - dt_fs: timestep in femtoseconds\n",
    "    - truncation_length: number of points to keep from the VACF before applying the Hann window.\n",
    "                         If None, use the full VACF length.\n",
    "    \n",
    "    Returns:\n",
    "    - freq_cm: frequencies in cm^-1\n",
    "    - ps_real_only: square of real part of FFT\n",
    "    - abs_part: absolute value of FFT (useful for standard power spectrum)\n",
    "    \"\"\"\n",
    "    \n",
    "    if truncation_length is None or truncation_length > len(vacf):\n",
    "        truncation_length = len(vacf)\n",
    "\n",
    "    vacf_cut = vacf[:truncation_length]\n",
    "    \n",
    "    # Apply Hann window\n",
    "    window = hann(truncation_length)\n",
    "    vacf_windowed = vacf_cut * window\n",
    "\n",
    "    # Compute FFT\n",
    "    spec = np.fft.rfft(vacf_windowed)\n",
    "\n",
    "    real_part = np.abs(spec)**2\n",
    "    abs_part = np.abs(spec)\n",
    "\n",
    "    # Frequency axis in fs^-1\n",
    "    freq_fs_inv = np.fft.rfftfreq(truncation_length, d=dt_fs)\n",
    "\n",
    "    # Convert to cm^-1\n",
    "    factor_fs_inv_to_cm = 3.335640951e4\n",
    "    freq_cm = freq_fs_inv * factor_fs_inv_to_cm\n",
    "\n",
    "    return freq_cm, real_part, abs_part\n",
    "\n",
    "def apply_qcf(freq, spectrum, temperature):\n",
    "    \"\"\"\n",
    "    Apply a quantum correction factor (QCF) to the power spectrum.\n",
    "   \n",
    "    Parameters:\n",
    "        freq (np.ndarray): Frequency array from FFT.\n",
    "        spectrum (np.ndarray): Power spectrum to be corrected.\n",
    "        temperature (float): Temperature in Kelvin (for quantum correction).\n",
    "       \n",
    "    Returns:\n",
    "        np.ndarray: Corrected power spectrum.\n",
    "    \"\"\"\n",
    "    hbar = 1.0545718e-34  # Planck's constant (J·s)\n",
    "    kB = 1.380649e-23     # Boltzmann constant (J/K)\n",
    "    beta = hbar * freq / (kB * temperature)\n",
    "    correction = beta / (1 - np.exp(-beta))\n",
    "    corrected_spectrum = spectrum * correction\n",
    "    return corrected_spectrum\n",
    "\n",
    "def gaussian_broaden(freq_cm, spectrum, fwhm_cm):\n",
    "    \"\"\"\n",
    "    Convolve 'spectrum' with a Gaussian of a given FWHM in cm^-1.\n",
    "\n",
    "    freq_cm : 1D array of frequencies in cm^-1 (assumed evenly spaced)\n",
    "    spectrum: 1D array (same length as freq_cm)\n",
    "    fwhm_cm : Desired full-width at half-maximum (FWHM) in cm^-1 for broadening\n",
    "\n",
    "    Returns:\n",
    "      spectrum_broadened : 1D array (same length as freq_cm) \n",
    "                           after applying Gaussian broadening\n",
    "    \"\"\"\n",
    "    # 1) Determine the spacing between frequency points (should be constant if using rfft)\n",
    "    df = freq_cm[1] - freq_cm[0]  # cm^-1 step\n",
    "\n",
    "    # 2) Convert the user-specified FWHM in cm^-1 to a Gaussian sigma (std dev) in cm^-1\n",
    "    #    For a Gaussian: FWHM = 2.35482 * sigma\n",
    "    sigma_cm = fwhm_cm / 2.35482\n",
    "\n",
    "    # 3) Convert sigma from cm^-1 to array indices\n",
    "    sigma_pts = sigma_cm / df\n",
    "\n",
    "    # 4) Use scipy's 1D Gaussian filter (which operates in array index space)\n",
    "    #    mode='reflect' or 'nearest' are typical choices to handle edges\n",
    "    spectrum_broadened = gaussian_filter1d(spectrum, sigma=sigma_pts, mode='reflect')\n",
    "\n",
    "    return spectrum_broadened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c7c6e",
   "metadata": {},
   "source": [
    "## Analysis Functions \n",
    "## 6 Heat Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b158a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lammps_log(filename, time_step=None):\n",
    "    \"\"\"Read a LAMMPS log file and return (header, data array).\"\"\"\n",
    "    data_lines, header = [], None\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            if lines[i].strip().startswith('Step'):\n",
    "                header = lines[i].strip().split()\n",
    "                i += 1\n",
    "                while i < len(lines) and lines[i].strip():\n",
    "                    tokens = lines[i].split()\n",
    "                    if len(tokens) != len(header):\n",
    "                        break\n",
    "                    try:\n",
    "                        data_lines.append([float(tok) for tok in tokens])\n",
    "                    except ValueError:\n",
    "                        break\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "    if not data_lines:\n",
    "        raise ValueError(f\"No data found in {filename}\")\n",
    "    data = np.array(data_lines)\n",
    "    # add Time column if missing\n",
    "    if \"Time\" not in header:\n",
    "        if \"Step\" not in header:\n",
    "            raise ValueError(\"'Step' column not found in log file.\")\n",
    "        if time_step is None:\n",
    "            time_step = 0.0005  # ps per step, adjust if needed\n",
    "        step_idx = header.index(\"Step\")\n",
    "        time_array = data[:, step_idx] * time_step\n",
    "        header.append(\"Time\")\n",
    "        data = np.hstack((data, time_array.reshape(-1, 1)))\n",
    "    return header, data\n",
    "\n",
    "# ---------------- Main analysis ----------------\n",
    "def compute_cp(\n",
    "    log_files,\n",
    "    n_molecules=510,\n",
    "    molecular_weight=18.01528,\n",
    "    quantum_corr=6.0,\n",
    "    fit=\"poly\",\n",
    "    degree=3,\n",
    "    plot=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute Cp(T) from a list of LAMMPS log files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_files : list of str\n",
    "        Paths to log files.\n",
    "    n_molecules : int\n",
    "        Number of molecules in the box.\n",
    "    molecular_weight : float\n",
    "        Molecular weight in g/mol (default water).\n",
    "    quantum_corr : float\n",
    "        Quantum correction (cal/mol/K) to subtract at 298 K.\n",
    "    fit : str\n",
    "        'poly' for polynomial fit, 'spline' for UnivariateSpline.\n",
    "    degree : int\n",
    "        Polynomial degree (ignored if fit='spline').\n",
    "    plot : bool\n",
    "        If True, make a Cp vs T plot.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    T_fine : ndarray\n",
    "        Temperature grid.\n",
    "    cp_fine : ndarray\n",
    "        Cp(T) in cal/mol/K.\n",
    "    \"\"\"\n",
    "\n",
    "    T_list, H_list = [], []\n",
    "\n",
    "    for path in log_files:\n",
    "        hdr, data = read_lammps_log(path)\n",
    "        cut = len(data) // 2  # discard equilibration\n",
    "        T0   = data[cut:, hdr.index(\"Temp\")].mean()\n",
    "        P0   = data[cut:, hdr.index(\"Press\")].mean()\n",
    "        rho0 = data[cut:, hdr.index(\"Density\")].mean()\n",
    "        E0_eV= data[cut:, hdr.index(\"E_pair\")].mean()\n",
    "\n",
    "        # Energy per molecule (eV → kcal/mol)\n",
    "        eV_per_mol = E0_eV / n_molecules\n",
    "        J_per_mol  = eV_per_mol * 1.602176634e-19 * 6.02214076e23\n",
    "        E0_kcal    = J_per_mol / 4184.0\n",
    "\n",
    "        # PV term (kcal/mol)\n",
    "        V_mol_cm3 = molecular_weight / rho0\n",
    "        PV_kcal   = P0 * V_mol_cm3 * 0.101325 / 4184.0\n",
    "\n",
    "        H0_kcal = E0_kcal + PV_kcal\n",
    "        T_list.append(T0)\n",
    "        H_list.append(H0_kcal * 1000.0)  # → cal/mol\n",
    "\n",
    "    # Sort by T\n",
    "    T_arr, H_arr = np.array(T_list), np.array(H_list)\n",
    "    order = np.argsort(T_arr)\n",
    "    T_arr, H_arr = T_arr[order], H_arr[order]\n",
    "\n",
    "    # Fit H(T)\n",
    "    T_fine = np.linspace(T_arr.min(), T_arr.max(), 400)\n",
    "    if fit == \"poly\":\n",
    "        coefs = np.polyfit(T_arr, H_arr, degree)\n",
    "        H_fine = np.polyval(coefs, T_fine)\n",
    "        cp_fine = np.polyval(np.polyder(coefs), T_fine)\n",
    "    elif fit == \"spline\":\n",
    "        spl = UnivariateSpline(T_arr, H_arr, k=5, s=1)\n",
    "        H_fine = spl(T_fine)\n",
    "        cp_fine = spl.derivative()(T_fine)\n",
    "    else:\n",
    "        raise ValueError(\"fit must be 'poly' or 'spline'\")\n",
    "\n",
    "    # Quantum correction near 298 K\n",
    "    i298 = np.abs(T_fine - 298.15).argmin()\n",
    "    cp_fine[i298] -= quantum_corr\n",
    "\n",
    "    # Plot\n",
    "    if plot:\n",
    "        plt.figure(figsize=(3.3, 2.8), dpi=300)\n",
    "        plt.plot(T_fine, cp_fine, label=\"Classical\")\n",
    "        plt.xlabel(\"T (K)\")\n",
    "        plt.ylabel(r\"$c_p$ (cal/mol K)\")\n",
    "        plt.legend(frameon=False, fontsize=8)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return T_fine, cp_fine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d4509",
   "metadata": {},
   "source": [
    "## Analysis Functions \n",
    "## 7 Thermal Expansion Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert T and rho lists from npt equilibrations\n",
    "T_list = []\n",
    "rho_list = []\n",
    "# convert to sorted numpy arrays (they’re already in ascending T, but this is a safeguard)\n",
    "T_arr   = np.array(T_list)\n",
    "rho_arr = np.array(rho_list)\n",
    "order   = np.argsort(T_arr)\n",
    "T_arr   = T_arr[order]\n",
    "rho_arr = rho_arr[order]\n",
    "\n",
    "# ————— spline fit & derivative —————\n",
    "# choose a small smoothing factor s if your data are noisy\n",
    "spl_rho     = UnivariateSpline(T_arr, rho_arr, k=5, s=1)\n",
    "drho_dT     = spl_rho.derivative()(T_arr)   # (g/cm^3)/K\n",
    "alphaP      = - drho_dT / rho_arr          # in K^(-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mace_openmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
